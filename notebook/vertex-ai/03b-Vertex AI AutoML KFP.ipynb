{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = '<YOUR_PROJECT_ID>'\n",
    "REGION = '<YOUR_REGION>'\n",
    "DATANAME = 'fraud'\n",
    "NOTEBOOK = '03b'\n",
    "\n",
    "# Resources\n",
    "DEPLOY_COMPUTE = 'n1-standard-2'\n",
    "\n",
    "# Model Training\n",
    "VAR_TARGET = 'Class'\n",
    "VAR_OMIT = 'transaction_id' # add more variables to the string with space delimiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from datetime import datetime\n",
    "import kfp\n",
    "from kfp.v2 import compiler\n",
    "#import kfp.v2.dsl as dsl\n",
    "#import google_cloud_pipeline_components as gcc_aip\n",
    "from google_cloud_pipeline_components.v1.dataset import TabularDatasetCreateOp\n",
    "from google_cloud_pipeline_components.v1.automl.training_job import AutoMLTabularTrainingJobRunOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "bq = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET = PROJECT_ID\n",
    "URI = f\"gs://{BUCKET}/{DATANAME}/models/{NOTEBOOK}\"\n",
    "DIR = f\"temp/{NOTEBOOK}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = !gcloud config list --format='value(core.account)' \n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {DIR}\n",
    "!mkdir -p {DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name = f'kfp-{NOTEBOOK}-{DATANAME}-{TIMESTAMP}',\n",
    "    pipeline_root = URI+'/'+str(TIMESTAMP)+'/kfp/'\n",
    ")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    dataname: str,\n",
    "    display_name: str,\n",
    "    deploy_machine: str,\n",
    "    bq_source: str,\n",
    "    var_target: str,\n",
    "    var_omit: str,\n",
    "    features: dict,\n",
    "    labels: dict \n",
    "):\n",
    "    \n",
    "    # dataset\n",
    "    dataset = TabularDatasetCreateOp(\n",
    "        project = project,\n",
    "        display_name = display_name,\n",
    "        bq_source = bq_source,\n",
    "        labels = labels\n",
    "    )\n",
    "    \n",
    "    # training\n",
    "    model = AutoMLTabularTrainingJobRunOp(\n",
    "        project = project,\n",
    "        display_name = display_name,\n",
    "        optimization_prediction_type = \"classification\",\n",
    "        optimization_objective = \"maximize-au-prc\",\n",
    "        budget_milli_node_hours = 1000,\n",
    "        disable_early_stopping=False,\n",
    "        column_specs = features,\n",
    "        dataset = dataset.outputs['dataset'],\n",
    "        target_column = var_target,\n",
    "        predefined_split_column_name = 'splits',\n",
    "        labels = labels\n",
    "    )\n",
    "    \n",
    "    # Endpoint: Creation\n",
    "    endpoint = EndpointCreateOp(\n",
    "        project = project,\n",
    "        display_name = display_name,\n",
    "        labels = labels\n",
    "    )\n",
    "    \n",
    "    # Endpoint: Deployment of Model\n",
    "    deployment = ModelDeployOp(\n",
    "        model = model.outputs[\"model\"],\n",
    "        endpoint = endpoint.outputs[\"endpoint\"],\n",
    "        dedicated_resources_min_replica_count = 1,\n",
    "        dedicated_resources_max_replica_count = 1,\n",
    "        traffic_split = {\"0\": 100},\n",
    "        dedicated_resources_machine_type= deploy_machine\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func = pipeline,\n",
    "    package_path = f\"{DIR}/{NOTEBOOK}.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp {DIR}/{NOTEBOOK}.json {URI}/{TIMESTAMP}/kfp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature names\n",
    "query = f\"SELECT * FROM {DATANAME}.INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{DATANAME}_prepped'\"\n",
    "schema = bq.query(query).to_dataframe()\n",
    "OMIT = VAR_OMIT.split() + [VAR_TARGET, 'splits']\n",
    "features = schema[~schema.column_name.isin(OMIT)].column_name.tolist()\n",
    "features = dict.fromkeys(features, 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name = f'{NOTEBOOK}_{DATANAME}_{TIMESTAMP}',\n",
    "    template_path = f\"{URI}/{TIMESTAMP}/kfp/{NOTEBOOK}.json\",\n",
    "    parameter_values = {\n",
    "        \"project\" : PROJECT_ID,\n",
    "        \"dataname\" : DATANAME,\n",
    "        \"display_name\" : f'{NOTEBOOK}_{DATANAME}_{TIMESTAMP}',\n",
    "        \"deploy_machine\" : DEPLOY_COMPUTE,\n",
    "        \"bq_source\" : f'bq://{PROJECT_ID}.{DATANAME}.{DATANAME}_prepped',\n",
    "        \"var_target\" : VAR_TARGET,\n",
    "        \"var_omit\" : VAR_OMIT,\n",
    "        \"features\" : features,\n",
    "        \"labels\" : {'notebook': NOTEBOOK}       \n",
    "    },\n",
    "    labels = {'notebook': NOTEBOOK},\n",
    "    enable_caching=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pipeline.run(\n",
    "    service_account = SERVICE_ACCOUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Review the Pipeline as it runs here:\\nhttps://console.cloud.google.com/vertex-ai/locations/{REGION}/pipelines/runs/{pipeline.resource_name.split('/')[-1]}?project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.get_pipeline_df(pipeline = f'kfp-{NOTEBOOK}-{DATANAME}-{TIMESTAMP}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
