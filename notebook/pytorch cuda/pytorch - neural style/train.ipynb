{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import vgg\n",
    "import transformer\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL SETTINGS\n",
    "TRAIN_IMAGE_SIZE = 256\n",
    "DATASET_PATH = \"/content/train\"\n",
    "NUM_EPOCHS = 1\n",
    "STYLE_IMAGE_PATH = \"/content/mosaic.jpg\"\n",
    "BATCH_SIZE = 4 \n",
    "CONTENT_WEIGHT = 17\n",
    "STYLE_WEIGHT = 50\n",
    "TV_WEIGHT = 1e-6 \n",
    "ADAM_LR = 0.001\n",
    "SAVE_MODEL_PATH = \"/content/\"\n",
    "SAVE_IMAGE_PATH = \"/content/\"\n",
    "SAVE_MODEL_EVERY = 500 # 2,000 Images with batch size 4\n",
    "SEED = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Seeds\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "    # Device\n",
    "    device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Dataset and Dataloader\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(TRAIN_IMAGE_SIZE),\n",
    "        transforms.CenterCrop(TRAIN_IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    train_dataset = datasets.ImageFolder(DATASET_PATH, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Load networks\n",
    "    TransformerNetwork = transformer.TransformerNetwork().to(device)\n",
    "    VGG = vgg.VGG16('/content/vgg16-00b39a1b.pth').to(device)\n",
    "\n",
    "    # Get Style Features\n",
    "    imagenet_neg_mean = torch.tensor([-103.939, -116.779, -123.68], dtype=torch.float32).reshape(1,3,1,1).to(device)\n",
    "    imagenet_mean = torch.tensor([103.939, 116.779, 123.68], dtype=torch.float32).reshape(1,3,1,1).to(device)\n",
    "    style_image = utils.load_image(STYLE_IMAGE_PATH)\n",
    "    style_tensor = utils.itot(style_image).to(device)\n",
    "    style_tensor = style_tensor.add(imagenet_neg_mean)\n",
    "    B, C, H, W = style_tensor.shape\n",
    "    style_features = VGG(style_tensor.expand([BATCH_SIZE, C, H, W]))\n",
    "    style_gram = {}\n",
    "    for key, value in style_features.items():\n",
    "        style_gram[key] = utils.gram(value)\n",
    "\n",
    "    # Optimizer settings\n",
    "    optimizer = optim.Adam(TransformerNetwork.parameters(), lr=ADAM_LR)\n",
    "\n",
    "    # Loss trackers\n",
    "    content_loss_history = []\n",
    "    style_loss_history = []\n",
    "    total_loss_history = []\n",
    "    batch_content_loss_sum = 0\n",
    "    batch_style_loss_sum = 0\n",
    "    batch_total_loss_sum = 0\n",
    "\n",
    "    # Optimization/Training Loop\n",
    "    batch_count = 1\n",
    "    start_time = time.time()\n",
    "    for epoch in range (1, NUM_EPOCHS+1):\n",
    "        print(\"========Epoch {}/{}========\".format(epoch, NUM_EPOCHS+1))\n",
    "        for batch_id, (content_batch, _) in enumerate(train_loader):\n",
    "            # Current Batch size in case of odd batches\n",
    "            curr_batch_size = content_batch.shape[0]\n",
    "            \n",
    "            # Zero-out Gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Generate images and get features\n",
    "            content_batch = content_batch[:,[2,1,0]].to(device)\n",
    "            generated_batch = TransformerNetwork(content_batch)\n",
    "            content_features = VGG(content_batch.add(imagenet_neg_mean))\n",
    "            generated_features = VGG(generated_batch.add(imagenet_neg_mean))\n",
    "\n",
    "            # Content Loss\n",
    "            MSELoss = nn.MSELoss().to(device)\n",
    "            content_loss = CONTENT_WEIGHT * MSELoss(content_features['relu2_2'], generated_features['relu2_2'])            \n",
    "            batch_content_loss_sum += content_loss\n",
    "\n",
    "            # Style Loss\n",
    "            style_loss = 0\n",
    "            for key, value in generated_features.items():\n",
    "                s_loss = MSELoss(utils.gram(value), style_gram[key][:curr_batch_size])\n",
    "                style_loss += s_loss\n",
    "            style_loss *= STYLE_WEIGHT\n",
    "            batch_style_loss_sum += style_loss\n",
    "\n",
    "            # Total Loss\n",
    "            total_loss = content_loss + style_loss\n",
    "            batch_total_loss_sum += total_loss.item()\n",
    "\n",
    "            # Backprop and Weight Update\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save Model and Print Losses\n",
    "            if (((batch_count-1)%SAVE_MODEL_EVERY == 0) or (batch_count==NUM_EPOCHS*len(train_loader))):\n",
    "                # Print Losses\n",
    "                print(\"========Iteration {}/{}========\".format(batch_count, NUM_EPOCHS*len(train_loader)))\n",
    "                print(\"\\tContent Loss:\\t{:.2f}\".format(batch_content_loss_sum/batch_count))\n",
    "                print(\"\\tStyle Loss:\\t{:.2f}\".format(batch_style_loss_sum/batch_count))\n",
    "                print(\"\\tTotal Loss:\\t{:.2f}\".format(batch_total_loss_sum/batch_count))\n",
    "                print(\"Time elapsed:\\t{} seconds\".format(time.time()-start_time))\n",
    "\n",
    "                # Save Model\n",
    "                checkpoint_path = SAVE_MODEL_PATH + \"checkpoint_\" + str(batch_count-1) + \".pth\"\n",
    "                torch.save(TransformerNetwork.state_dict(), checkpoint_path)\n",
    "                print(\"Saved TransformerNetwork checkpoint file at {}\".format(checkpoint_path))\n",
    "\n",
    "                # Save sample generated image\n",
    "                sample_tensor = generated_batch[0].clone().detach().unsqueeze(dim=0)\n",
    "                sample_image = utils.ttoi(sample_tensor.clone().detach())\n",
    "                sample_image_path = SAVE_IMAGE_PATH + \"sample0_\" + str(batch_count-1) + \".png\"\n",
    "                utils.saveimg(sample_image, sample_image_path)\n",
    "                utils.show(sample_image)\n",
    "                print(\"Saved sample tranformed image at {}\".format(sample_image_path))\n",
    "\n",
    "                # Save loss histories\n",
    "                content_loss_history.append(batch_total_loss_sum/batch_count)\n",
    "                style_loss_history.append(batch_style_loss_sum/batch_count)\n",
    "                total_loss_history.append(batch_total_loss_sum/batch_count)\n",
    "\n",
    "            # Iterate Batch Counter\n",
    "            batch_count+=1\n",
    "\n",
    "    stop_time = time.time()\n",
    "    # Print loss histories\n",
    "    print(\"Done Training the Transformer Network!\")\n",
    "    print(\"Training Time: {} seconds\".format(stop_time-start_time))\n",
    "    print(\"========Content Loss========\")\n",
    "    print(content_loss_history) \n",
    "    print(\"========Style Loss========\")\n",
    "    print(style_loss_history) \n",
    "    print(\"========Total Loss========\")\n",
    "    print(total_loss_history) \n",
    "\n",
    "    # Save TransformerNetwork weights\n",
    "    TransformerNetwork.eval()\n",
    "    TransformerNetwork.cpu()\n",
    "    final_path = SAVE_MODEL_PATH + \"transformer_weight.pth\"\n",
    "    print(\"Saving TransformerNetwork weights at {}\".format(final_path))\n",
    "    torch.save(TransformerNetwork.state_dict(), final_path)\n",
    "    print(\"Done saving final model\")\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
